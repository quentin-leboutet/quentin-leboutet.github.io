<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Quentin Leboutet</title>

  <meta name="author" content="Quentin Leboutet">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Quentin Leboutet
                  </p>
                  <p>I'm a research scientist at <a href="https://www.intel.com/">Intel</a> in Munich, specializing in
                    the generation of 3D and <a href="https://quentin-leboutet.github.io/MIDGArD/">articulated
                      assets</a> using diffusion models.
                  </p>
                  <p>
                    At Intel I also contributed to the development of the <a
                      href="https://github.com/spear-sim/spear">SPEAR</a> photorealistic
                    simulator, the <a href="https://github.com/isl-org/Open3D">Open3D</a> library and the <a
                      href="https://github.com/isl-org/OpenBot">OpenBot</a> framework. I completed my PhD in Electrical
                    Engineering and Computer Science at the <a href="https://www.tum.de/">Technical University of
                      Munich</a>, under the guidance of <a href="https://www.professoren.tum.de/cheng-gordon">Prof.
                      Gordon Cheng</a></a>.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:quentin.leboutet@intel.com">Email</a> &nbsp;/&nbsp;
                    <a href="data/QuentinLeboutet-CV.pdf">CV</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?hl=en&user=SfiqI4AAAAAJ">Scholar</a> &nbsp;/&nbsp;
                    <a href="https://de.linkedin.com/in/quentinleboutet">Linkedin</a> &nbsp;/&nbsp;
                    <a href="https://github.com/quentin-leboutet/">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/QuentinLeboutet.png"><img
                      style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo"
                      src="images/QuentinLeboutet.png" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Research</h2>
                  <p>
                    I'm interested in generative AI, deep learning, agentic AI, and robotics. My <a
                      href="https://mediatum.ub.tum.de/doc/1638193/document.pdf">PhD research</a> focused on Robot
                    Control, Tactile feedback, State Estimation and Inertial Parameters Identification.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>


              <tr onmouseout="ever_stop()" onmouseover="ever_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/midgard.jpg" alt="MIDGArD" width=100% style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://quentin-leboutet.github.io/MIDGArD/">
                    <span class="papertitle">MIDGArD: Modular Interpretable Diffusion over Graphs for Articulated
                      Designs
                    </span>
                  </a>
                  <br>
                  <strong>Quentin Leboutet</strong>,
                  <a href="https://scholar.google.com/citations?user=qC1JKzoAAAAJ&hl=en&oi=ao">Nina Wiedemann</a>,
                  <a href="https://scholar.google.com/citations?user=AZNUIDAAAAAJ&hl=en&oi=ao">Zhipeng Cai</a>,
                  <a href="https://scholar.google.com/citations?user=Xdy_LywAAAAJ&hl=en&oi=ao">Michael Paulitsch</a>,
                  <a href="https://scholar.google.com/citations?user=8eLlbhMAAAAJ&hl=en&oi=ao">Kai Yuan</a>.
                  <br>
                  <em>NeurIPS</em>, 2024
                  <br>
                  <a href="https://quentin-leboutet.github.io/MIDGArD/">Project page</a>
                  /
                  <a href="https://openreview.net/pdf?id=re2jPCnzkA">Paper</a>
                  /
                  <a href="https://github.com/quentin-leboutet/MIDGArD">Code</a>
                  <p></p>
                  <p>
                    MIDGArD is a modular diffusion framework that generates articulated 3D assets with enhanced
                    controllability, enabling seamless integration into physics engines for advanced digital content and
                    robotics applications.
                  </p>
                </td>
              </tr>

              <tr onmouseout="cat3d_stop()" onmouseover="cat3d_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/insight.jpg" alt="InSight" width=100% style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/pdf/2408.00343">
                    <span class="papertitle">In-Sight: Interactive Navigation through Sight
                    </span>
                  </a>
                  <br>
                  <a href="https://www.linkedin.com/in/philipp-schoch-421758233/">Philipp Schoch</a>,
                  <a href="https://scholar.google.com/citations?user=R3OeSRoAAAAJ&hl=en&oi=ao">Fan Yang</a>,
                  <a href="https://scholar.google.com/citations?user=do0paQsAAAAJ&hl=en">Yuntao Ma</a>,
                  <a href="https://scholar.google.com/citations?user=SmGQ48gAAAAJ&hl=en&oi=ao">Stefan Leutenegger</a>,
                  <a href="https://scholar.google.com/citations?user=DO3quJYAAAAJ&hl=en&oi=ao">Marco Hutter</a>,
                  <strong>Quentin Leboutet</strong>.
                  <br>
                  <em>IEEE IROS</em>, 2024
                  <br>
                  <a href="https://arxiv.org/pdf/2408.00343">Paper</a>
                  /
                  <a href="https://www.youtube.com/watch?v=ja0Vjm72ZDw">Video</a>
                  <p></p>
                  <p>
                    IN-Sight is a self-supervised navigation system that supports interaction with obstacles using RGB-D
                    data, enabling robots such as ANYmal to seamlessly navigate complex real-world environments.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/openbot.jpg" alt="OpenBot" width=100% style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://github.com/isl-org/OpenBot">
                    <span class="papertitle">OpenBot-Fleet: A System for Collective Learning with Real Robots
                    </span>
                  </a>
                  <br>
                  <a href="https://matthias.pw/">Matthias Müller</a>,
                  <a href="https://scholar.google.com/citations?user=4Me5raoAAAAJ&hl=en&oi=ao">Samarth Brahmbhatt</a>,
                  <a href="https://scholar.google.com/citations?user=CaBIO8cAAAAJ&hl=en&oi=ao">Ankur Deka</a>,
                  <strong>Quentin Leboutet</strong>,
                  <a href="https://scholar.google.com/citations?user=ls_kE0UAAAAJ&hl=en&oi=ao">David Hafner</a>,
                  <a href="https://vladlen.info/">Vladlen Koltun</a>.

                  <br>
                  <em>IEEE ICRA</em>, 2024
                  <br>
                  <a href="https://arxiv.org/pdf/2405.07515">Paper</a>
                  /
                  <a href="https://www.youtube.com/watch?v=3tuKzbtdFcc">Video</a>
                  /
                  <a href="https://drive.google.com/file/d/1pMDeX7CiOlk3iRgK8thWS0aAEgrtGsIb/view">Supplementary
                    Material</a>
                  <p></p>
                  <p>
                    OpenBot-Fleet is a scalable open-source cloud robotics system that leverages smartphones and
                    affordable wheeled robots to learn and deploy robust navigation policies across real-world
                    environments.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/birdy.jpg" alt="BIRDy" width=100% style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://www.mdpi.com/2076-3417/11/9/4303">
                    <span class="papertitle">Inertial Parameter Identification in Robotics: A Survey</span>
                  </a>
                  <br>
                  <strong>Quentin Leboutet</strong>,
                  <a href="https://scholar.google.com/citations?user=atWYHuIAAAAJ&hl=en&oi=sra">Julien Roux</a>,
                  <a href="https://scholar.google.com/citations?user=dMsp7pIAAAAJ&hl=en&oi=ao">Alexandre Janot</a>,
                  <a href="https://scholar.google.com/citations?user=dnF9A3AAAAAJ&hl=en&oi=sra">Julio Rogelio Guadarrama
                    Olvera</a>,
                  <a href="https://scholar.google.com/citations?user=km_K9awAAAAJ&hl=en&oi=ao">Gordon Cheng</a>.
                  <br>
                  <em>Applied Sciences</em>, 2021, <strong>Best Paper Award</strong>
                  <br>
                  <a href="https://www.mdpi.com/2076-3417/11/9/4303">Paper</a>
                  /
                  <a href="https://github.com/TUM-ICS/BIRDy">Code</a>
                  /
                  <a href="https://zenodo.org/records/4679467">Supplementary Material</a>
                  /
                  <a href="https://zenodo.org/records/4728085">Data</a>
                  <p>Introducing BIRDy: an open-source Matlab toolbox that benchmarks 17 cutting-edge inertial parameter
                    identification methods, enabling precise robot dynamics analysis for both simulated and real-world
                    manipulators.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/icra2020.jpg" alt="BIRDy" width=100% style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/9197169">
                    <span class="papertitle">Second-order Kinematics for Floating-Base Robots using the Redundant
                      Acceleration Feedback of an Artificial Sensory Skin</span>
                  </a>
                  <br>
                  <strong>Quentin Leboutet</strong>,
                  <a href="https://scholar.google.com/citations?user=dnF9A3AAAAAJ&hl=en&oi=sra">Julio Rogelio Guadarrama
                    Olvera</a>,
                  <a href="https://scholar.google.com/citations?user=abELbgsAAAAJ&hl=en&oi=sra">Florian Bergner</a>,
                  <a href="https://scholar.google.com/citations?user=km_K9awAAAAJ&hl=en&oi=ao">Gordon Cheng</a>.
                  <br>
                  <em>IEEE ICRA</em>, 2020
                  <br>
                  <a href="https://ieeexplore.ieee.org/abstract/document/9197169">Paper</a>
                  <p>A second-order kinematics estimation method that utilizes distributed inertial feedback and
                    self-calibrating artificial skin to measure joint motions in humanoid robots, alleviating noise and
                    lag issues.</p>
                </td>
              </tr>



              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/iros2020.jpg" alt="BIRDy" width=100% style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://mediatum.ub.tum.de/doc/1579859/document.pdf">
                    <span class="papertitle">Online Configuration Selection for Redundant Arrays of Inertial Sensors:
                      Application to Robotic Systems Covered with a Multimodal Artificial Skin</span>
                  </a>
                  <br>
                  <strong>Quentin Leboutet</strong>,
                  <a href="https://scholar.google.com/citations?user=abELbgsAAAAJ&hl=en&oi=sra">Florian Bergner</a>,
                  <a href="https://scholar.google.com/citations?user=km_K9awAAAAJ&hl=en&oi=ao">Gordon Cheng</a>.
                  <br>
                  <em>IEEE IROS</em>, 2020
                  <br>
                  <a href="https://mediatum.ub.tum.de/doc/1579859/document.pdf">Paper</a>
                  <p>An adaptive sensor-selection algorithm that dynamically optimizes inertial sensor usage on robots
                    in real-time, enhancing scalability and robustness for high-order motion estimation in dynamic
                    control applications.</p>
                </td>
              </tr>



              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/ieee2019.jpg" alt="BIRDy" width=100% style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812712">
                    <span class="papertitle">A Comprehensive Realization of Robot Skin: Sensors, Sensing, Control, and
                      Applications</span>
                  </a>
                  <br>
                  <a href="https://scholar.google.com/citations?user=km_K9awAAAAJ&hl=en&oi=ao">Gordon Cheng</a>,
                  <a href="https://scholar.google.com/citations?user=lF0BpyIAAAAJ&hl=en&oi=sra">Emmanuel Carlos Dean
                    León</a>,
                  <a href="https://scholar.google.com/citations?user=abELbgsAAAAJ&hl=en&oi=sra">Florian Bergner</a>,
                  <a href="https://scholar.google.com/citations?user=dnF9A3AAAAAJ&hl=en&oi=sra">Julio Rogelio Guadarrama
                    Olvera</a>,
                  <strong>Quentin Leboutet</strong>,
                  <a href="https://scholar.google.com/citations?user=BWThiiQAAAAJ&hl=en&oi=ao">Philipp Mittendorfer</a>.
                  <br>
                  <em>Proceedings of the IEEE</em>, 2019
                  <br>
                  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812712">Paper</a>
                  <p>A holistic approach to engineer the artificial skin for robots with an example of a multimodal skin
                    cell showing multiple humanlike sensing modalities.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/tro2019.jpg" alt="BIRDy" width=100% style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://mediatum.ub.tum.de/doc/1474210/document.pdf">
                    <span class="papertitle">Tactile-Based Whole-Body Compliance with Force Propagation for Mobile
                      Manipulators</span>
                  </a>
                  <br>
                  <strong>Quentin Leboutet</strong>,
                  <a href="https://scholar.google.com/citations?user=lF0BpyIAAAAJ&hl=en&oi=sra">Emmanuel Carlos Dean
                    León</a>,
                  <a href="https://scholar.google.com/citations?user=abELbgsAAAAJ&hl=en&oi=sra">Florian Bergner</a>,
                  <a href="https://scholar.google.com/citations?user=km_K9awAAAAJ&hl=en&oi=ao">Gordon Cheng</a>.
                  <br>
                  <em>IEEE TRO</em>, 2019
                  <br>
                  <a href="https://mediatum.ub.tum.de/doc/1474210/document.pdf">Paper</a>
                  /
                  <a href="https://www.youtube.com/watch?v=teKbiiSN2K8">Video</a>
                  <p>A quadratic programming-based tactile control framework that equips mobile robots with whole-body
                    compliance through adaptive artificial skin, enabling robust and adjustable responses to
                    multi-contact environmental interactions.</p>
                </td>
              </tr>


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/humanoids2016.jpg" alt="Humanoids2016" width=100% style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://mediatum.ub.tum.de/doc/1336140/document.pdf">
                    <span class="papertitle">Tactile-Based Compliance with Hierarchical Force Propagation for
                      Omnidirectional Mobile Manipulators</span>
                  </a>
                  <br>
                  <strong>Quentin Leboutet</strong>,
                  <a href="https://scholar.google.com/citations?user=lF0BpyIAAAAJ&hl=en&oi=sra">Emmanuel Carlos Dean
                    León</a>,
                  <a href="https://scholar.google.com/citations?user=km_K9awAAAAJ&hl=en&oi=ao">Gordon Cheng</a>.
                  <br>
                  <em>IEEE Humanoids</em>, 2016
                  <br>
                  <a href="https://mediatum.ub.tum.de/doc/1336140/document.pdf">Paper</a>
                  /
                  <a href="https://www.youtube.com/watch?v=hLhkJx7xsg8">Video</a>
                  <p>A quadratic programming-based framework that empowers omnidirectional mobile robots with whole-body
                    compliance, utilizing artificial skin's tactile feedback to enable robust and prioritized
                    interactions with complex environments.</p>
                </td>
              </tr>
            </tbody>
          </table>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <h2>Teaching Activities</h2>
                </td>
              </tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellpadding="10">
            <tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/ics.jpg" alt="ICS" width=100%>
                </td>
                <td width="75%" valign="center">
                  <a
                    href="https://campus.tum.de/tumonline/ee/ui/ca2/app/desktop/#/slc.tm.cp/student/courses/950373140?$scrollTo=toc_overview">Graduate
                    Student Instructor,
                    Humanoid Robotics Systems (2017 – 2021)</a>
                  <br>
                  <a
                    href="https://campus.tum.de/tumonline/ee/ui/ca2/app/desktop/#/slc.tm.cp/student/courses/950344472?$scrollTo=toc_overview">Graduate
                    Student Instructor,
                    Humanoid Sensors and Actuators (2017 – 2021)</a>
                  <br>
                  <a
                    href="https://campus.tum.de/tumonline/ee/ui/ca2/app/desktop/#/slc.tm.cp/student/courses/950432958?$scrollTo=toc_overview">Graduate
                    Student Instructor,
                    Humanoid Robo-Cup (2018 – 2021)</a>
                  <br>
                  <a
                    href="https://campus.tum.de/tumonline/ee/ui/ca2/app/desktop/#/slc.tm.cp/student/courses/950764498?$scrollTo=toc_overview">Graduate
                    Student Instructor,
                    Multi-sensory based robot dynamic manipulation (2016 – 2018)</a>
                  <br>
                </td>
              </tr>

            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    Credits to <a href="https://github.com/jonbarron/jonbarron_website"></a>Jon Barron</a> for the
                    website source code and <a href="https://leonidk.com/">Leonid Keselman</a> for the Jekyll import.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>